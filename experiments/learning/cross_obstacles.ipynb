{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Test script for single agent problems.\n",
    "\n",
    "This scripts runs the best model found by one of the executions of `singleagent.py`\n",
    "\n",
    "Example\n",
    "-------\n",
    "To run the script, type in a terminal:\n",
    "\n",
    "    $ python test_singleagent.py --exp ./results/save-<env>-<algo>-<obs>-<act>-<time_date>\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import re\n",
    "import numpy as np\n",
    "import gym\n",
    "import pprint\n",
    "import torch\n",
    "\n",
    "from gym_pybullet_drones.utils.utils import sync\n",
    "from gym_pybullet_drones.utils.Logger import Logger\n",
    "from gym_pybullet_drones.envs.single_agent_rl import NavigateLandAviary\n",
    "from gym_pybullet_drones.envs.single_agent_rl import CrossObstaclesAviary\n",
    "from gym_pybullet_drones.envs.single_agent_rl import map_name_to_env\n",
    "\n",
    "from gym_pybullet_drones.envs.single_agent_rl.BaseSingleAgentAviary import (\n",
    "    ActionType,\n",
    "    ObservationType,\n",
    ")\n",
    "import pybullet as p\n",
    "#from gym_pybullet_drones.envs.multi_agent_rl import MultiCrossObstaclesAviary\n",
    "\n",
    "import shared_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#env_name = \"land-aviary-v0\"\n",
    "env_name = \"multicrossobs-aviary-v0\"\n",
    "# env_name = \"long-cross-obs-aviary-v0\"\n",
    "# env_name = \"cross-obstacles-aviary-v0\"\n",
    "\n",
    "OBS = (\n",
    "    ObservationType[\"KIN\"]\n",
    ")\n",
    "ACT = ActionType[\"RPM\"]\n",
    "#### Evaluate the model ####################################\n",
    "env = gym.make(\n",
    "    env_name,\n",
    "    aggregate_phy_steps=shared_constants.AGGR_PHY_STEPS,\n",
    "    obs=OBS,\n",
    "    act=ACT,\n",
    "    gui=True,\n",
    "    #difficulty=16,\n",
    "    num_drones=6,\n",
    ")\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = {\"__all__\": False}\n",
    "while not done[\"__all__\"]:\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    time.sleep(0.05)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    if done[\"__all__\"]:\n",
    "        env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.loadURDF(\"cube.urdf\", [0, 0, 0.5], physicsClientId=env.CLIENT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Single Step of env and print state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "action = env.action_space.sample()\n",
    "for i in range(1):\n",
    "    time.sleep(0.05)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(f\"action: {action}\")\n",
    "    print(f\"obs: {obs}\")\n",
    "    print(f\"reward: {reward}\")\n",
    "    print(f\"done: {done}\")\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "action = env.action_space.sample()\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try and flip\n",
    "action = env.action_space.sample()\n",
    "print(action)\n",
    "for i in range(100):\n",
    "    time.sleep(0.05)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    #state = env.getFirstDroneState()\n",
    "    print(f\"action: {action}\")\n",
    "    print(f\"obs: {obs}\")\n",
    "    print(f\"reward: {reward}\")\n",
    "    print(f\"done: {done}\")\n",
    "    #print(f\"state: {state}\")\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "landing_zone_xyz = [3.5, 3.5, 0.0625]\n",
    "position = state[0:3]\n",
    "print(position)\n",
    "target_position = landing_zone_xyz\n",
    "pos_dist = np.linalg.norm(position[0:2] - target_position[0:2])\n",
    "print(pos_dist)\n",
    "\n",
    "y_dist = np.linalg.norm(position[2] - (target_position[2] - 0.1))\n",
    "print(y_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rpy = state[7:10]\n",
    "print(rpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change initial position of the drones\n",
    "a = p.getContactPoints(bodyA=env.DRONE_IDS[0],\n",
    "            physicsClientId=env.CLIENT\n",
    "            )\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "p.resetSimulation()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('drones')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "368184befd70de6859feb7ba7ed007b2fb115c321767e95f8823607adf146467"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
