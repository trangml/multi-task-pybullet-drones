# general script settings
tag: difficulty_0
env: long-cross-obs
#choices=["maze", "hover", "obstacles", "land", "field", "land-vision"],
algo: ppo
#choices=["a2c", "ppo", "sac", "td3", "ddpg"],
obs: DEPTH
# choices=["kin", "rgb", "both"],
act: RPM
exp: none
cpu: 8
n_steps: 1e7

# PPO arguments
ppo:
  #normalize: true
  #choices=["true", "false"],
  gae_lambda: 0.95
  gamma: 0.99
  ent_coef: 0.0
  learning_rate: 2.5e-4
  #clip_rate: 0.2
  policy_kwargs:
    log_std_init: -1
    ortho_init: False
    activation_fn:
      _partial_: true
      _target_: torch.nn.LeakyReLU
    #net_arch: [512, 256, { "pi": [256, 256], "vf": [256, 256] }]
    # Agents do better with the default net_arch
    # net_arch:
    #   - 512
    #   - 256
    #   - "pi": [256, 256]
    #     "vf": [256, 256]

# Env specific settings
env_kwargs:
  initial_xyzs: [[-0.5, 0, 0.5]]

  bounds: [[12, 1, 1], [-1, -1, 0.1]]
  difficulty: 1
  collision_detection: True

  # rewards
  reward_components:
    EnterAreaReward:
      scale: 40.0
      area: [[11, 12], [-1, 1]]
    OrientationReward:
      scale: 0.01
    IncreaseXRewardV2:
      scale: 0.1
    BoundsReward:
      scale: 10.0
      bounds: ${env_kwargs.bounds}
  # terminals
  term_components:
    # EnterAreaTerm:
    #   area: [[4, 5], [-1, 1]]
    BoundsTerm:
      bounds: ${env_kwargs.bounds}
    OrientationTerm:
